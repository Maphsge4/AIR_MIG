=> model params: 124439808
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
Test: [   0/1000]	Time  1.522 ( 1.522)
292709376
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
Test: [  10/1000]	Time  1.268 ( 1.287)
292709376
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
Test: [  20/1000]	Time  1.260 ( 1.274)
292709376
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912
0
12582912
1
12582912
2
12582912
3
12582912
4
12582912
5
12582912
6
12582912
7
12582912
8
12582912
9
12582912
10
12582912
11
12582912

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 30:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per gpu:                                               124.44 M
params of model = params per GPU * mp_size:                   124.44 M
fwd MACs per GPU:                                             3092.38 GMACs
fwd flops per GPU:                                            6190.25 G
fwd flops of model = fwd flops per GPU * mp_size:             6190.25 G
fwd latency:                                                  1.61 s  
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          3.85 TFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'GPT2Model': '124.44 M'}
    MACs        - {'GPT2Model': '3092.38 GMACs'}
    fwd latency - {'GPT2Model': '1.61 s'}
depth 1:
    params      - {'ModuleList': '85.05 M'}
    MACs        - {'ModuleList': '1546.19 GMACs'}
    fwd latency - {'OffloadModel': '1.59 s'}
depth 2:
    params      - {'GPT2Block': '85.05 M'}
    MACs        - {'GPT2Block': '1546.19 GMACs'}
    fwd latency - {'Sequential': '853.15 ms'}
depth 3:
    params      - {'ModelShard': '85.05 M'}
    MACs        - {'ModelShard': '1546.19 GMACs'}
    fwd latency - {'ModelShard': '853.15 ms'}
depth 4:
    params      - {'GPT2Block': '85.05 M'}
    MACs        - {'GPT2Block': '1546.19 GMACs'}
    fwd latency - {'GPT2Block': '823.4 ms'}
depth 5:
    params      - {'GPT2MLP': '56.67 M'}
    MACs        - {'GPT2MLP': '927.71 GMACs'}
    fwd latency - {'GPT2MLP': '375.5 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

GPT2Model(
  124.44 M, 100.00% Params, 3092.38 GMACs, 100.00% MACs, 1.61 s, 100.00% latency, 3.85 TFLOPS, 
  (wte): Embedding(38.6 M, 31.02% Params, 0 MACs, 0.00% MACs, 178.18 us, 0.01% latency, 0.0 FLOPS, 50257, 768)
  (wpe): Embedding(786.43 k, 0.63% Params, 0 MACs, 0.00% MACs, 14.34 us, 0.00% latency, 0.0 FLOPS, 1024, 768)
  (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 67.5 ms, 4.20% latency, 3.82 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 104.45 us, 0.01% latency, 602.35 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.94 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.66 ms, 0.35% latency, 10.25 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.22 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.33 ms, 1.95% latency, 4.94 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.4 ms, 0.58% latency, 8.23 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.54 ms, 0.34% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 68.86 ms, 4.29% latency, 3.75 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.63 ms, 0.10% latency, 38.68 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.27 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.24 ms, 1.94% latency, 4.95 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.31 ms, 0.58% latency, 8.3 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.53 ms, 0.34% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.25 ms, 4.31% latency, 3.72 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.96 ms, 0.12% latency, 32.12 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.29 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.26 us, 0.01% latency, 653.62 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.3 ms, 1.95% latency, 4.94 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.34 ms, 0.58% latency, 8.28 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.56 ms, 0.35% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.44 ms, 4.32% latency, 3.71 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 2.11 ms, 0.13% latency, 29.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.65 ms, 0.35% latency, 10.27 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.34 ms, 1.95% latency, 4.93 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.39 ms, 0.58% latency, 8.23 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.55 ms, 0.35% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.4 ms, 4.32% latency, 3.72 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 2.1 ms, 0.13% latency, 30.0 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.89 ms, 1.61% latency, 3.99 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.31 ms, 1.95% latency, 4.94 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.36 ms, 0.58% latency, 8.26 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.55 ms, 0.35% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 68.79 ms, 4.28% latency, 3.75 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.51 ms, 0.09% latency, 41.71 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.89 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.3 ms, 1.95% latency, 4.94 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.36 ms, 0.58% latency, 8.26 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.53 ms, 0.34% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (6): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.04 ms, 4.30% latency, 3.74 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.77 ms, 0.11% latency, 35.53 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.89 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.28 ms, 1.95% latency, 4.94 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.37 ms, 0.58% latency, 8.25 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.51 ms, 0.34% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (7): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 67.56 ms, 4.21% latency, 3.82 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 332.32 us, 0.02% latency, 189.32 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.65 ms, 0.35% latency, 10.27 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.18 us, 0.01% latency, 675.16 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.25 ms, 1.94% latency, 4.95 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.32 ms, 0.58% latency, 8.29 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.52 ms, 0.34% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (8): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.41 ms, 4.32% latency, 3.72 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 2.16 ms, 0.13% latency, 29.07 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.26 ms, 1.95% latency, 4.95 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.34 ms, 0.58% latency, 8.28 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.53 ms, 0.34% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (9): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 68.64 ms, 4.27% latency, 3.76 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.35 ms, 0.08% latency, 46.72 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.3 ms, 1.95% latency, 4.94 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.4 ms, 0.59% latency, 8.23 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.51 ms, 0.34% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (10): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 67.78 ms, 4.22% latency, 3.81 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 441.95 us, 0.03% latency, 142.36 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.91 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.27 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.34 ms, 1.95% latency, 4.93 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.35 ms, 0.58% latency, 8.27 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.59 ms, 0.35% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
    (11): GPT2Block(
      7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 67.73 ms, 4.22% latency, 3.81 TFLOPS, 
      (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 492.35 us, 0.03% latency, 127.78 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.89 ms, 1.61% latency, 3.98 TFLOPS, 
        (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.27 TFLOPS, )
        (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
        (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
        (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
      (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.25 ms, 1.95% latency, 4.95 TFLOPS, 
        (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
        (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.32 ms, 0.58% latency, 8.3 TFLOPS, )
        (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.54 ms, 0.34% latency, 0.0 FLOPS, )
        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.8 ms, 0.11% latency, 35.03 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
  (hh): OffloadModel(
    85.05 M, 68.35% Params, 1546.19 GMACs, 50.00% MACs, 1.59 s, 99.16% latency, 1.94 TFLOPS, 
    (_model): Sequential(
      85.05 M, 68.35% Params, 1546.19 GMACs, 50.00% MACs, 853.15 ms, 53.11% latency, 3.63 TFLOPS, 
      (0): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.94 ms, 4.35% latency, 3.69 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 67.5 ms, 4.20% latency, 3.82 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 104.45 us, 0.01% latency, 602.35 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.94 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.66 ms, 0.35% latency, 10.25 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.22 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.33 ms, 1.95% latency, 4.94 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.4 ms, 0.58% latency, 8.23 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.54 ms, 0.34% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (1): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 71.34 ms, 4.44% latency, 3.62 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 68.86 ms, 4.29% latency, 3.75 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.63 ms, 0.10% latency, 38.68 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.27 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.24 ms, 1.94% latency, 4.95 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.31 ms, 0.58% latency, 8.3 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.53 ms, 0.34% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (2): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 71.73 ms, 4.46% latency, 3.6 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.25 ms, 4.31% latency, 3.72 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.96 ms, 0.12% latency, 32.12 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.29 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.26 us, 0.01% latency, 653.62 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.3 ms, 1.95% latency, 4.94 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.34 ms, 0.58% latency, 8.28 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.56 ms, 0.35% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (3): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 71.92 ms, 4.48% latency, 3.59 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.44 ms, 4.32% latency, 3.71 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 2.11 ms, 0.13% latency, 29.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.65 ms, 0.35% latency, 10.27 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.34 ms, 1.95% latency, 4.93 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.39 ms, 0.58% latency, 8.23 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.55 ms, 0.35% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (4): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 71.88 ms, 4.47% latency, 3.59 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.4 ms, 4.32% latency, 3.72 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 2.1 ms, 0.13% latency, 30.0 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.89 ms, 1.61% latency, 3.99 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.31 ms, 1.95% latency, 4.94 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.36 ms, 0.58% latency, 8.26 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.55 ms, 0.35% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (5): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 71.28 ms, 4.44% latency, 3.62 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 68.79 ms, 4.28% latency, 3.75 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.51 ms, 0.09% latency, 41.71 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.89 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.3 ms, 1.95% latency, 4.94 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.36 ms, 0.58% latency, 8.26 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.53 ms, 0.34% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (6): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 71.52 ms, 4.45% latency, 3.61 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.04 ms, 4.30% latency, 3.74 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.77 ms, 0.11% latency, 35.53 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.89 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.28 ms, 1.95% latency, 4.94 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.37 ms, 0.58% latency, 8.25 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.51 ms, 0.34% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (7): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 70.05 ms, 4.36% latency, 3.68 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 67.56 ms, 4.21% latency, 3.82 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 332.32 us, 0.02% latency, 189.32 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.65 ms, 0.35% latency, 10.27 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.18 us, 0.01% latency, 675.16 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.25 ms, 1.94% latency, 4.95 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.32 ms, 0.58% latency, 8.29 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.52 ms, 0.34% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (8): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 71.89 ms, 4.48% latency, 3.59 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 69.41 ms, 4.32% latency, 3.72 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 2.16 ms, 0.13% latency, 29.07 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.26 ms, 1.95% latency, 4.95 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.34 ms, 0.58% latency, 8.28 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.53 ms, 0.34% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (9): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 71.12 ms, 4.43% latency, 3.63 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 68.64 ms, 4.27% latency, 3.76 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 1.35 ms, 0.08% latency, 46.72 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.9 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.28 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.3 ms, 1.95% latency, 4.94 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.4 ms, 0.59% latency, 8.23 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.51 ms, 0.34% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (10): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 70.26 ms, 4.37% latency, 3.67 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 67.78 ms, 4.22% latency, 3.81 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 441.95 us, 0.03% latency, 142.36 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.91 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.27 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.71 ms, 0.23% latency, 5.21 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.23 us, 0.01% latency, 660.65 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.34 ms, 1.95% latency, 4.93 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.65 ms, 0.41% latency, 11.62 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.35 ms, 0.58% latency, 8.27 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.59 ms, 0.35% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
      (11): ModelShard(
        7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 70.21 ms, 4.37% latency, 3.67 TFLOPS, 
        (model_shard): GPT2Block(
          7.09 M, 5.70% Params, 128.85 GMACs, 4.17% MACs, 67.73 ms, 4.22% latency, 3.81 TFLOPS, 
          (ln_1): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 492.35 us, 0.03% latency, 127.78 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            2.36 M, 1.90% Params, 51.54 GMACs, 1.67% MACs, 25.89 ms, 1.61% latency, 3.98 TFLOPS, 
            (c_attn): Conv1D(1.77 M, 1.42% Params, 28.99 GMACs, 0.94% MACs, 5.64 ms, 0.35% latency, 10.27 TFLOPS, )
            (c_proj): Conv1D(590.59 k, 0.47% Params, 9.66 GMACs, 0.31% MACs, 3.7 ms, 0.23% latency, 5.22 TFLOPS, )
            (attn_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
            (resid_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 3.07 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
          (ln_2): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.21 us, 0.01% latency, 667.83 GFLOPS, (768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            4.72 M, 3.79% Params, 77.31 GMACs, 2.50% MACs, 31.25 ms, 1.95% latency, 4.95 TFLOPS, 
            (c_fc): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 6.66 ms, 0.41% latency, 11.61 TFLOPS, )
            (c_proj): Conv1D(2.36 M, 1.90% Params, 38.65 GMACs, 1.25% MACs, 9.32 ms, 0.58% latency, 8.3 TFLOPS, )
            (act): NewGELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 5.54 ms, 0.34% latency, 0.0 FLOPS, )
            (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 4.1 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)
          )
        )
      )
    )
  )
)
------------------------------------------------------------------------------
Test: [  30/1000]	Time  1.646 ( 1.281)
292709376
